--- /home/sgr/PycharmProjects/caption/models.py
+++ /home/sgr/PycharmProjects/caption/models.py
@@ -1,25 +1,6 @@
 class DecoderWithAttention(nn.Module):
-    """
-    Decoder.
-    """
-
-    def __init__(
-        self,
-        attention_dim,
-        embed_dim,
-        decoder_dim,
-        vocab_size,
-        encoder_dim=2048,
-        dropout=0.5,
-    ):
-        """
-        :param attention_dim: size of attention network
-        :param embed_dim: embedding size
-        :param decoder_dim: size of decoder's RNN
-        :param vocab_size: size of vocabulary
-        :param encoder_dim: feature size of encoded images
-        :param dropout: dropout
-        """
+    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, decoder_layers=1, encoder_dim=2048,
+                 dropout=0.5):
         super(DecoderWithAttention, self).__init__()
 
         self.encoder_dim = encoder_dim
@@ -27,100 +8,58 @@
         self.embed_dim = embed_dim
         self.decoder_dim = decoder_dim
         self.vocab_size = vocab_size
+        self.decoder_layers = decoder_layers
         self.dropout = dropout
 
-        self.attention = Attention(
-            encoder_dim, decoder_dim, attention_dim
-        )  # attention network
+        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)
 
-        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer
+        self.embedding = nn.Embedding(vocab_size, embed_dim)
         self.dropout = nn.Dropout(p=self.dropout)
-        self.decode_step = nn.LSTMCell(
-            embed_dim + encoder_dim, decoder_dim, bias=True
-        )  # decoding LSTMCell
-        self.init_h = nn.Linear(
-            encoder_dim, decoder_dim
-        )  # linear layer to find initial hidden state of LSTMCell
-        self.init_c = nn.Linear(
-            encoder_dim, decoder_dim
-        )  # linear layer to find initial cell state of LSTMCell
-        self.f_beta = nn.Linear(
-            decoder_dim, encoder_dim
-        )  # linear layer to create a sigmoid-activated gate
+        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, decoder_layers)
+        self.init_h = nn.Linear(encoder_dim, decoder_dim)
+        self.init_c = nn.Linear(encoder_dim, decoder_dim)
+        self.f_beta = nn.Linear(decoder_dim, encoder_dim)
         self.sigmoid = nn.Sigmoid()
-        self.fc = nn.Linear(
-            decoder_dim, vocab_size
-        )  # linear layer to find scores over vocabulary
-        self.init_weights()  # initialize some layers with the uniform distribution
+        self.fc = nn.Linear(decoder_dim, vocab_size)
+        self.init_weights()
 
     def init_weights(self):
-        """
-        Initializes some parameters with values from the uniform distribution, for easier convergence.
-        """
         self.embedding.weight.data.uniform_(-0.1, 0.1)
         self.fc.bias.data.fill_(0)
         self.fc.weight.data.uniform_(-0.1, 0.1)
 
     def load_pretrained_embeddings(self, embeddings):
-        """
-        Loads embedding layer with pre-trained embeddings.
-
-        :param embeddings: pre-trained embeddings
-        """
         self.embedding.weight = nn.Parameter(embeddings)
 
     def fine_tune_embeddings(self, fine_tune=True):
-        """
-        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).
-
-        :param fine_tune: Allow?
-        """
         for p in self.embedding.parameters():
             p.requires_grad = fine_tune
 
     def init_hidden_state(self, encoder_out):
-        """
-        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.
-
-        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)
-        :return: hidden state, cell state
-        """
         mean_encoder_out = encoder_out.mean(dim=1)
         h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)
         c = self.init_c(mean_encoder_out)
         return h, c
 
     def forward(self, encoder_out, encoded_captions, caption_lengths):
-        """
-        Forward propagation.
-
-        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)
-        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)
-        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)
-        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices
-        """
-
+        # encoder_out.shape = (batch_size, image_size, image_size, encoder_dim), image_size being the pixel width/height
+        # encoded_captions.shape = (batch_size, max_caption_length)
+        # caption_lengths.shape = (batch_size, 1)
         batch_size = encoder_out.size(0)
         encoder_dim = encoder_out.size(-1)
         vocab_size = self.vocab_size
 
         # Flatten image
-        encoder_out = encoder_out.view(
-            batch_size, -1, encoder_dim
-        )  # (batch_size, num_pixels, encoder_dim)
+        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)
         num_pixels = encoder_out.size(1)
 
         # Sort input data by decreasing lengths; why? apparent below
-        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(
-            dim=0, descending=True
-        )
+        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)
         encoder_out = encoder_out[sort_ind]
         encoded_captions = encoded_captions[sort_ind]
 
         # Embedding
-        embeddings = self.embedding(
-            encoded_captions
-        )  # (batch_size, max_caption_length, embed_dim)
+        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)
 
         # Initialize LSTM state
         h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)
@@ -130,9 +69,7 @@
         decode_lengths = (caption_lengths - 1).tolist()
 
         # Create tensors to hold word predicion scores and alphas
-        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(
-            device
-        )
+        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)
         alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)
 
         # At each time-step, decode by
@@ -140,20 +77,13 @@
         # then generate a new word in the decoder with the previous word and the attention weighted encoding
         for t in range(max(decode_lengths)):
             batch_size_t = sum([l > t for l in decode_lengths])
-            attention_weighted_encoding, alpha = self.attention(
-                encoder_out[:batch_size_t], h[:batch_size_t]
-            )
-            gate = self.sigmoid(
-                self.f_beta(h[:batch_size_t])
-            )  # gating scalar, (batch_size_t, encoder_dim)
+            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],
+                                                                h[:batch_size_t])
+            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)
             attention_weighted_encoding = gate * attention_weighted_encoding
             h, c = self.decode_step(
-                torch.cat(
-                    [embeddings[:batch_size_t, t, :], attention_weighted_encoding],
-                    dim=1,
-                ),
-                (h[:batch_size_t], c[:batch_size_t]),
-            )  # (batch_size_t, decoder_dim)
+                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),
+                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)
             preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)
             predictions[:batch_size_t, t, :] = preds
             alphas[:batch_size_t, t, :] = alpha